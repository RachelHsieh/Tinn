# Setting Up Your Excel Processor in GitHub Codespaces

## Step 1: Create a GitHub Repository

1. Go to [GitHub.com](https://github.com) and sign in
2. Click the "+" button in the top right corner
3. Select "New repository"
4. Name it something like `excel-processor-tool`
5. Make it **Public** (for free Codespaces) or **Private** (if you have GitHub Pro)
6. Check "Add a README file"
7. Click "Create repository"

## Step 2: Prepare Your Repository Files

You'll need to create several files in your repository. Here's the structure:

```
your-repo/
â”œâ”€â”€ .devcontainer/
â”‚   â””â”€â”€ devcontainer.json
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ NPIV2.py
â”œâ”€â”€ web_interface.py
â””â”€â”€ README.md
```

### File 1: `.devcontainer/devcontainer.json`
Create a folder called `.devcontainer` and inside it, create `devcontainer.json`:

```json
{
    "name": "Excel Processor Environment",
    "image": "mcr.microsoft.com/devcontainers/python:3.11",
    "features": {
        "ghcr.io/devcontainers/features/common-utils:2": {}
    },
    "customizations": {
        "vscode": {
            "extensions": [
                "ms-python.python",
                "ms-python.pylint"
            ]
        }
    },
    "postCreateCommand": "pip install -r requirements.txt",
    "forwardPorts": [8080],
    "portsAttributes": {
        "8080": {
            "label": "Excel Processor Web Interface",
            "onAutoForward": "openBrowser"
        }
    }
}
```

### File 2: `requirements.txt`
```txt
pandas>=1.5.0
openpyxl>=3.0.0
numpy>=1.24.0
streamlit>=1.28.0
```

### File 3: `.gitignore`
```txt
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Virtual environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Excel files (optional - remove if you want to commit sample files)
*.xlsx
*.xls

# Streamlit
.streamlit/
```

## Step 3: Create a Web Interface

Since Codespaces doesn't have a desktop GUI, let's create a web-based interface using Streamlit.

Create `web_interface.py`:

```python
import streamlit as st
import pandas as pd
import os
import tempfile
from io import BytesIO
import sys

# Add the current directory to Python path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Import your existing processor (we'll modify it slightly)
from excel_processor_web import ExcelProcessorWeb

def main():
    st.set_page_config(
        page_title="Excel Data Processor",
        page_icon="ðŸ“Š",
        layout="wide"
    )
    
    st.title("ðŸ“Š Excel Data Processor with Order Quantities")
    st.markdown("---")
    
    # Initialize session state
    if 'processor' not in st.session_state:
        st.session_state.processor = ExcelProcessorWeb()
    
    # File upload section
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("1. Upload Main Excel File")
        main_file = st.file_uploader(
            "Choose your main Excel file",
            type=['xlsx', 'xls'],
            key="main_file"
        )
    
    with col2:
        st.subheader("2. Upload Order Excel File")
        order_file = st.file_uploader(
            "Choose your order Excel file",
            type=['xlsx', 'xls'],
            key="order_file"
        )
    
    # Process files when both are uploaded
    if main_file is not None and order_file is not None:
        st.markdown("---")
        
        if st.button("ðŸš€ Process Files", type="primary"):
            with st.spinner("Processing files..."):
                try:
                    # Save uploaded files temporarily
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx') as tmp_main:
                        tmp_main.write(main_file.getvalue())
                        main_path = tmp_main.name
                    
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx') as tmp_order:
                        tmp_order.write(order_file.getvalue())
                        order_path = tmp_order.name
                    
                    # Process the files
                    result = st.session_state.processor.process_files(main_path, order_path)
                    
                    if result['success']:
                        st.success("âœ… Files processed successfully!")
                        
                        # Display summary
                        st.subheader("ðŸ“ˆ Processing Summary")
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            st.metric("Total Items", result['total_items'])
                        with col2:
                            st.metric("Items with Order Qty", result['matched_items'])
                        with col3:
                            st.metric("Match Rate", f"{result['match_rate']:.1f}%")
                        
                        # Download processed file
                        st.subheader("ðŸ“¥ Download Results")
                        
                        # Convert to bytes for download
                        output_buffer = BytesIO()
                        with pd.ExcelWriter(output_buffer, engine='openpyxl') as writer:
                            result['summary_df'].to_excel(writer, sheet_name='Summary', index=False)
                            result['combined_df'].to_excel(writer, sheet_name='Combined', index=False)
                        
                        st.download_button(
                            label="ðŸ“¥ Download Processed File",
                            data=output_buffer.getvalue(),
                            file_name=f"{main_file.name.split('.')[0]}_processed.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        )
                        
                        # Preview data
                        st.subheader("ðŸ‘€ Data Preview")
                        st.dataframe(result['combined_df'].head(10))
                        
                    else:
                        st.error(f"âŒ Error: {result['error']}")
                    
                    # Clean up temp files
                    os.unlink(main_path)
                    os.unlink(order_path)
                    
                except Exception as e:
                    st.error(f"âŒ An error occurred: {str(e)}")
    
    # Instructions
    with st.expander("ðŸ“– How to Use"):
        st.markdown("""
        1. **Upload Main Excel File**: Your primary Excel file containing the Summary sheet and data sheets
        2. **Upload Order Excel File**: Excel file containing Item and Order Quantity columns
        3. **Click Process**: The tool will process both files and combine the data
        4. **Download Results**: Get your processed Excel file with order quantities added
        
        **Requirements for files:**
        - Main file should have a 'Summary' sheet with 'Issue key' and 'Summary' columns
        - Order file should have columns named 'Item' (or similar) and 'Order Quantity' (or similar)
        - Data sheets should contain: Planner, Published, Item Number, Item Description, Oracle On Hand
        """)

if __name__ == "__main__":
    main()
```

## Step 4: Modify Your Original Script for Web Use

Create `excel_processor_web.py` (modified version of your original script):

```python
import pandas as pd
import numpy as np
from typing import Dict, Any, List

class ExcelProcessorWeb:
    def __init__(self):
        self.summary_lookup = {}
        self.order_quantity_lookup = {}
    
    def process_files(self, main_file_path: str, order_file_path: str) -> Dict[str, Any]:
        """Process both files and return results"""
        try:
            # Process order file
            if not self._process_order_file(order_file_path):
                return {
                    'success': False,
                    'error': 'Failed to process order file'
                }
            
            # Process main file
            workbook = pd.ExcelFile(main_file_path)
            
            # Process summary sheet
            self._process_summary_sheet(main_file_path)
            
            # Process other sheets
            processed_sheets = self._process_other_sheets(main_file_path, workbook)
            
            if not processed_sheets:
                return {
                    'success': False,
                    'error': 'No sheets were processed successfully'
                }
            
            # Combine sheets
            combined_df = pd.concat(processed_sheets, ignore_index=True)
            summary_df = pd.read_excel(main_file_path, sheet_name='Summary')
            
            # Calculate statistics
            ordered_qty_count = combined_df['Ordered Qty'].apply(
                lambda x: pd.notna(x) and str(x) != "" and str(x) != "0"
            ).sum()
            total_items = len(combined_df)
            match_rate = (ordered_qty_count / total_items * 100) if total_items > 0 else 0
            
            return {
                'success': True,
                'combined_df': combined_df,
                'summary_df': summary_df,
                'total_items': total_items,
                'matched_items': int(ordered_qty_count),
                'match_rate': match_rate
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    def _process_order_file(self, order_file_path: str) -> bool:
        """Process the order file to create quantity lookup"""
        try:
            # Read the order file
            order_df = pd.read_excel(order_file_path, header=None)
            
            # Find columns (same logic as original)
            item_col = None
            order_qty_col = None
            header_row = None
            
            # Search for column headers
            for row_idx in range(min(15, len(order_df))):
                for col_idx in range(len(order_df.columns)):
                    if pd.notna(order_df.iloc[row_idx, col_idx]):
                        cell_value = str(order_df.iloc[row_idx, col_idx]).strip().lower()
                        
                        if cell_value in ["item", "item number", "item_number", "part", "part number"]:
                            item_col = col_idx
                            header_row = row_idx
                        elif cell_value in ["order quantity", "order qty", "quantity", "qty"]:
                            order_qty_col = col_idx
                            header_row = row_idx
                
                if item_col is not None and order_qty_col is not None:
                    break
            
            if item_col is None or order_qty_col is None:
                return False
            
            # Process the data
            item_quantities = {}
            for row_idx in range(header_row + 1, len(order_df)):
                item_value = order_df.iloc[row_idx, item_col]
                qty_value = order_df.iloc[row_idx, order_qty_col]
                
                if pd.notna(item_value) and pd.notna(qty_value):
                    item_str = str(item_value).strip()
                    try:
                        qty_num = float(qty_value)
                        item_quantities[item_str] = item_quantities.get(item_str, 0) + qty_num
                    except (ValueError, TypeError):
                        continue
            
            self.order_quantity_lookup = item_quantities
            return True
            
        except Exception as e:
            print(f"Error processing order file: {str(e)}")
            return False
    
    def _process_summary_sheet(self, file_path: str):
        """Process Summary sheet and create lookup dictionary"""
        try:
            summary_df = pd.read_excel(file_path, sheet_name='Summary', header=None)
            
            # Find Issue Key and Summary columns (same logic as original)
            issue_key_row = None
            issue_key_col = None
            
            for row_idx in range(len(summary_df)):
                for col_idx in range(len(summary_df.columns)):
                    if pd.notna(summary_df.iloc[row_idx, col_idx]) and \
                       str(summary_df.iloc[row_idx, col_idx]).strip() == "Issue key":
                        issue_key_row = row_idx
                        issue_key_col = col_idx
                        break
                if issue_key_row is not None:
                    break
            
            if issue_key_row is None:
                return
            
            # Find Summary column
            summary_col = None
            header_row = summary_df.iloc[issue_key_row]
            for col_idx in range(len(header_row)):
                if pd.notna(header_row.iloc[col_idx]) and \
                   str(header_row.iloc[col_idx]).strip() == "Summary":
                    summary_col = col_idx
                    break
            
            if summary_col is None:
                return
            
            # Create lookup dictionary
            for row_idx in range(issue_key_row + 1, len(summary_df)):
                issue_key = summary_df.iloc[row_idx, issue_key_col]
                summary_value = summary_df.iloc[row_idx, summary_col]
                
                if pd.notna(issue_key) and pd.notna(summary_value):
                    self.summary_lookup[str(issue_key).strip()] = str(summary_value).strip()
                    
        except Exception as e:
            print(f"Error processing Summary sheet: {str(e)}")
    
    def _process_other_sheets(self, file_path: str, workbook) -> List[pd.DataFrame]:
        """Process all sheets except Summary sheet"""
        processed_sheets = []
        required_columns = ["Planner", "Published", "Item Number", "Item Description", "Oracle On Hand"]
        
        for sheet_name in workbook.sheet_names:
            if sheet_name == 'Summary':
                continue
                
            try:
                df = pd.read_excel(file_path, sheet_name=sheet_name, header=None)
                
                # Get values from B1 and B2
                model_value = ""
                b2c_date_value = ""
                
                if len(df) > 0 and len(df.columns) > 1:
                    if pd.notna(df.iloc[0, 1]):
                        model_value = str(df.iloc[0, 1]).strip()
                
                if len(df) > 1 and len(df.columns) > 1:
                    if pd.notna(df.iloc[1, 1]):
                        b2c_date_value = str(df.iloc[1, 1]).strip()
                
                # Check A1 for Issue Key lookup
                if len(df) > 0 and len(df.columns) > 0:
                    a1_value = df.iloc[0, 0]
                    if pd.notna(a1_value):
                        a1_str = str(a1_value).strip()
                        if a1_str in self.summary_lookup:
                            model_value = self.summary_lookup[a1_str]
                
                # Find table boundaries (same logic as original)
                table_start_row = self._find_table_start(df, required_columns)
                
                if table_start_row is None:
                    continue
                
                # Process table data (same logic as original)
                processed_data = self._extract_table_data(
                    df, table_start_row, required_columns, model_value, b2c_date_value
                )
                
                if processed_data:
                    sheet_df = pd.DataFrame(processed_data)
                    sheet_df['Source_Sheet'] = sheet_name
                    processed_sheets.append(sheet_df)
                    
            except Exception as e:
                print(f"Error processing sheet {sheet_name}: {str(e)}")
        
        return processed_sheets
    
    def _find_table_start(self, df, required_columns):
        """Find the start of the data table"""
        for row_idx in range(len(df)):
            row_data = df.iloc[row_idx]
            found_columns = 0
            for col in required_columns:
                if any(str(cell).strip() == col for cell in row_data if pd.notna(cell)):
                    found_columns += 1
            
            if found_columns >= 2:
                return row_idx
        
        return None
    
    def _extract_table_data(self, df, table_start_row, required_columns, model_value, b2c_date_value):
        """Extract data from the table"""
        header_row = df.iloc[table_start_row]
        
        # Find column positions
        column_positions = {}
        for col_idx, cell_value in enumerate(header_row):
            if pd.notna(cell_value):
                cell_str = str(cell_value).strip()
                if cell_str in required_columns:
                    column_positions[cell_str] = col_idx
        
        # Extract data
        new_columns = ["Model", "B2C Date"] + required_columns + ["Ordered Qty"]
        processed_data = []
        
        for row_idx in range(table_start_row + 1, len(df)):
            row_data = {"Model": model_value, "B2C Date": b2c_date_value}
            
            has_data = False
            item_number = None
            
            for col_name in required_columns:
                if col_name in column_positions:
                    col_idx = column_positions[col_name]
                    if col_idx < len(df.columns):
                        cell_value = df.iloc[row_idx, col_idx]
                        row_data[col_name] = cell_value
                        if pd.notna(cell_value):
                            has_data = True
                            if col_name == "Item Number":
                                item_number = str(cell_value).strip()
                else:
                    row_data[col_name] = ""
            
            # Add ordered quantity
            ordered_qty = ""
            if item_number and item_number in self.order_quantity_lookup:
                ordered_qty = self.order_quantity_lookup[item_number]
            
            row_data["Ordered Qty"] = ordered_qty
            
            if has_data:
                processed_data.append(row_data)
        
        return processed_data
```

## Step 5: Upload Files to GitHub

1. Go to your GitHub repository
2. Click "uploading an existing file" or drag and drop
3. Upload all the files you created:
   - `NPIV2.py` (your original file)
   - `web_interface.py`
   - `excel_processor_web.py`
   - `requirements.txt`
   - `.gitignore`
   - Create the `.devcontainer` folder and upload `devcontainer.json`

## Step 6: Launch Codespace

1. In your GitHub repository, click the green "Code" button
2. Select the "Codespaces" tab
3. Click "Create codespace on main"
4. Wait for the environment to set up (2-3 minutes)

## Step 7: Run the Application

Once your Codespace is ready:

1. Open the terminal in Codespace
2. Run: `streamlit run web_interface.py --server.port 8080`
3. A browser tab should automatically open with your web interface
4. If not, click on the "Ports" tab and open port 8080

## Step 8: Share with Coworkers

Your coworkers can access this tool by:

1. Going to your GitHub repository
2. Clicking "Code" â†’ "Codespaces" â†’ "Create codespace"
3. Running `streamlit run web_interface.py --server.port 8080`

**Note**: Each person will need their own Codespace instance, but they'll all use the same code.

## Benefits of This Approach

âœ… **No Installation Required**: Coworkers don't need to install Python or any packages
âœ… **Cross-Platform**: Works on any device with a web browser
âœ… **Always Updated**: Everyone uses the latest version of your code
âœ… **Easy to Maintain**: Update code in one place
âœ… **Free Tier Available**: GitHub provides free Codespace hours

## GitHub Codespaces Limits

- **Free tier**: 120 core hours/month + 15 GB storage
- **Paid tier**: $0.18/hour for 2-core machines
- Codespaces automatically stop after 30 minutes of inactivity

This setup gives you a professional, web-based tool that your coworkers can use without any setup hassle!
